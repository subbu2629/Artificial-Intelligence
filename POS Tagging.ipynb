{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "we'll have to predict the Part of Speech (POS) tag for each word in a provided sentence in this work.\n",
    "\n",
    "You are required to build a model using Hidden Markov Models which would help you predict the POS tags for all words in an utterance.\n",
    "\n",
    "### What is a POS tag?\n",
    "\n",
    "In corpus linguistics, part-of-speech tagging (POS tagging or PoS tagging or POST), also called grammatical tagging or word-category disambiguation, is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on both its definition and its contextâ€”i.e., its relationship with adjacent and related words in a phrase, sentence, or paragraph. A simplified form of this is commonly taught to school-age children, in the identification of words as nouns, verbs, adjectives, adverbs, etc.\n",
    "\n",
    "\n",
    "#### Dataset Description\n",
    "##### Sample Tuple\n",
    "b100-5507\n",
    "\n",
    "Mr.\tNOUN\n",
    "<br>\n",
    "Podger\tNOUN\n",
    "<br>\n",
    "had\tVERB\n",
    "<br>\n",
    "thanked\tVERB\n",
    "<br>\n",
    "him\tPRON\n",
    "<br>\n",
    "gravely\tADV\n",
    "<br>\n",
    ",\t.\n",
    "<br>\n",
    "and\tCONJ\n",
    "<br>\n",
    "now\tADV\n",
    "<br>\n",
    "he\tPRON\n",
    "<br>\n",
    "made\tVERB\n",
    "<br>\n",
    "use\tNOUN\n",
    "<br>\n",
    "of\tADP\n",
    "<br>\n",
    "the\tDET\n",
    "<br>\n",
    "advice\tNOUN\n",
    "<br>\n",
    ".\t.\n",
    "<br>\n",
    "##### Explanation\n",
    "The first token \"b100-5507\" is just a key and acts like an identifier to indicate the beginning of a sentence.\n",
    "<br>\n",
    "The other tokens have a (Word, POS Tag) pairing.\n",
    "\n",
    "__List of POS Tags are:__\n",
    ".\n",
    "<br>\n",
    "ADJ\n",
    "<br>\n",
    "ADP\n",
    "<br>\n",
    "ADV\n",
    "<br>\n",
    "CONJ\n",
    "<br>\n",
    "DET\n",
    "<br>\n",
    "NOUN\n",
    "<br>\n",
    "NUM\n",
    "<br>\n",
    "PRON\n",
    "<br>\n",
    "PRT\n",
    "<br>\n",
    "VERB\n",
    "<br>\n",
    "X\n",
    "\n",
    "__Note__\n",
    "<br>\n",
    "__.__ is used to indicate special characters such as '.', ','\n",
    "<br>\n",
    "__X__ is used to indicate vocab not part of Enlish Language mostly.\n",
    "Others are Standard POS tags.\n",
    "\n",
    "\n",
    "### Train-Test Split\n",
    "Let us use a 80-20 split of our data for training and evaluation purpose.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict, namedtuple, OrderedDict\n",
    "from pomegranate import State, HiddenMarkovModel, DiscreteDistribution\n",
    "from itertools import chain\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentence = namedtuple(\"Sentence\", \"words tags\")\n",
    "\n",
    "def read_data(filename):\n",
    "    \"\"\"This is to read tagged sentence data\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        sentence_lines = [l.split(\"\\n\") for l in f.read().split(\"\\n\\n\")]\n",
    "    return OrderedDict(((s[0], Sentence(*zip(*[l.strip().split(\"\\t\")\n",
    "                        for l in s[1:]]))) for s in sentence_lines if s[0]))\n",
    "\n",
    "\n",
    "def read_tags(filename):\n",
    "    \"\"\"This is to read list of word tag classes\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        tags = f.read().split(\"\\n\")\n",
    "    return frozenset(tags)\n",
    "\n",
    "class Subset(namedtuple(\"BaseSet\", \"sentences keys vocab X tagset Y N stream\")):\n",
    "    def __new__(cls, sentences, keys):\n",
    "        word_sequences = tuple([sentences[k].words for k in keys])\n",
    "        tag_sequences = tuple([sentences[k].tags for k in keys])\n",
    "        wordset = frozenset(chain(*word_sequences))\n",
    "        tagset = frozenset(chain(*tag_sequences))\n",
    "        N = sum(1 for _ in chain(*(sentences[k].words for k in keys)))\n",
    "        stream = tuple(zip(chain(*word_sequences), chain(*tag_sequences)))\n",
    "        return super().__new__(cls, {k: sentences[k] for k in keys}, keys, wordset, word_sequences,\n",
    "                               tagset, tag_sequences, N, stream.__iter__)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.sentences.items())\n",
    "\n",
    "\n",
    "class Dataset(namedtuple(\"_Dataset\", \"sentences keys vocab X tagset Y training_set testing_set N stream\")):\n",
    "    def __new__(cls, tagfile, datafile, train_test_split=0.8, seed=112890):\n",
    "        tagset = read_tags(tagfile)\n",
    "        sentences = read_data(datafile)\n",
    "        keys = tuple(sentences.keys())\n",
    "        wordset = frozenset(chain(*[s.words for s in sentences.values()]))\n",
    "        word_sequences = tuple([sentences[k].words for k in keys])\n",
    "        tag_sequences = tuple([sentences[k].tags for k in keys])\n",
    "        N = sum(1 for _ in chain(*(s.words for s in sentences.values())))\n",
    "        \n",
    "        # split data into train/test sets\n",
    "        _keys = list(keys)\n",
    "        if seed is not None: random.seed(seed)\n",
    "        random.shuffle(_keys)\n",
    "        split = int(train_test_split * len(_keys))\n",
    "        training_data = Subset(sentences, _keys[:split])\n",
    "        testing_data = Subset(sentences, _keys[split:])\n",
    "        stream = tuple(zip(chain(*word_sequences), chain(*tag_sequences)))\n",
    "        return super().__new__(cls, dict(sentences), keys, wordset, word_sequences, tagset,\n",
    "                               tag_sequences, training_data, testing_data, N, stream.__iter__)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.sentences.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57340 sentences are there in the corpus.\n",
      "45872 sentences are there in the training set.\n",
      "11468 sentences are there in the testing set.\n"
     ]
    }
   ],
   "source": [
    "#Read data\n",
    " \n",
    "data = Dataset(\"D:\\\\M.Tech\\\\Sem 2\\\\AI\\\\tags.txt\", \"D:\\\\M.Tech\\\\Sem 2\\\\AI\\\\data.txt\", train_test_split=0.8)\n",
    "\n",
    "print(\"{} sentences are there in the corpus.\".format(len(data)))\n",
    "print(\"{} sentences are there in the training set.\".format(len(data.training_set)))\n",
    "print(\"{} sentences are there in the testing set.\".format(len(data.testing_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 1161192 samples of 56057 unique words in the corpus.\n",
      "There are 928458 samples of 50536 unique words in the training set.\n",
      "There are 232734 samples of 25112 unique words in the testing set.\n",
      "There are 5521 words in the test set that are missing in the training set.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are a total of {} samples of {} unique words in the corpus.\"\n",
    "      .format(data.N, len(data.vocab)))\n",
    "print(\"There are {} samples of {} unique words in the training set.\"\n",
    "      .format(data.training_set.N, len(data.training_set.vocab)))\n",
    "print(\"There are {} samples of {} unique words in the testing set.\"\n",
    "      .format(data.testing_set.N, len(data.testing_set.vocab)))\n",
    "print(\"There are {} words in the test set that are missing in the training set.\"\n",
    "      .format(len(data.testing_set.vocab - data.training_set.vocab)))\n",
    "\n",
    "assert data.N == data.training_set.N + data.testing_set.N, \\\n",
    "       \"The number of training + test samples should be equal to the total number of samples\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tStream (word, tag) pairs:\n",
      "\n",
      "\t ('Mr.', 'NOUN')\n",
      "\t ('Podger', 'NOUN')\n",
      "\t ('had', 'VERB')\n",
      "\t ('thanked', 'VERB')\n",
      "\t ('him', 'PRON')\n"
     ]
    }
   ],
   "source": [
    "# Dataset.stream() usage in getting (word, tag) samples for the entire corpus\n",
    "print(\"\\n\\tStream (word, tag) pairs:\\n\")\n",
    "for i, pair in enumerate(data.stream()):\n",
    "    print(\"\\t\", pair)\n",
    "    if i > 3: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagwordpair_counts(tags, words):\n",
    "    d = defaultdict(lambda: defaultdict(int))\n",
    "    for tag, word in zip(tags, words):\n",
    "        d[tag][word] += 1\n",
    "        \n",
    "    return d\n",
    "tags = [tag for i, (word, tag) in enumerate(data.training_set.stream())]\n",
    "words = [word for i, (word, tag) in enumerate(data.training_set.stream())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unknown_replace(sequence):\n",
    "    \n",
    "    return [w if w in data.training_set.vocab else 'nan' for w in sequence]\n",
    "\n",
    "def simplification_decoding(X, model):\n",
    "    \n",
    "    _, state_path = model.viterbi(unknown_replace(X))\n",
    "    return [state[1].name for state in state_path[1:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_counts(sequences):\n",
    "\n",
    "    return Counter(sequences)\n",
    "\n",
    "tags = [tag for i, (word, tag) in enumerate(data.training_set.stream())]\n",
    "tag_unigrams = unigram_counts(tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_counts(sequences):\n",
    "\n",
    "    d = Counter(sequences)\n",
    "    return d\n",
    "\n",
    "tags = [tag for i, (word, tag) in enumerate(data.stream())]\n",
    "o = [(tags[i],tags[i+1]) for i in range(0,len(tags)-2,2)]\n",
    "tag_bigrams = bigram_counts(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def starting_counts(sequences):\n",
    "    \n",
    "    d = Counter(sequences)\n",
    "    return d\n",
    "\n",
    "tags = [tag for i, (word, tag) in enumerate(data.stream())]\n",
    "starts_tag = [i[0] for i in data.Y]\n",
    "tag_starts = starting_counts(starts_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ending_counts(sequences):\n",
    "    \n",
    "    d = Counter(sequences)\n",
    "    return d\n",
    "\n",
    "end_tag = [i[len(i)-1] for i in data.Y]\n",
    "tag_ends = ending_counts(end_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def modelaccuracy(X, Y, model):\n",
    "    \n",
    "    correct = total_predictions = 0\n",
    "    for observations, actual_tags in zip(X, Y):\n",
    "        try:\n",
    "            most_likely_tags = simplification_decoding(observations, model)\n",
    "            correct += sum(p == t for p, t in zip(most_likely_tags, actual_tags))\n",
    "        except:\n",
    "            pass\n",
    "        total_predictions += len(observations)\n",
    "    return correct / total_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HMM Model Goes Here\n",
    "basic_model = HiddenMarkovModel(name=\"base-hmm-tagger\")\n",
    "\n",
    "tags = [tag for i, (word, tag) in enumerate(data.stream())]\n",
    "words = [word for i, (word, tag) in enumerate(data.stream())]\n",
    "\n",
    "tags_count=unigram_counts(tags)\n",
    "tag_words_count=tagwordpair_counts(tags,words)\n",
    "\n",
    "starting_tag_list=[i[0] for i in data.Y]\n",
    "ending_tag_list=[i[-1] for i in data.Y]\n",
    "\n",
    "starting_tag_count=starting_counts(starting_tag_list)#the number of times a tag occured at the start\n",
    "ending_tag_count=ending_counts(ending_tag_list)      #the number of times a tag occured at the end\n",
    "\n",
    "\n",
    "\n",
    "to_pass_states = []\n",
    "for tag, words_dict in tag_words_count.items():\n",
    "    total = float(sum(words_dict.values()))\n",
    "    distribution = {word: count/total for word, count in words_dict.items()}\n",
    "    tag_emissions = DiscreteDistribution(distribution)\n",
    "    tag_state = State(tag_emissions, name=tag)\n",
    "    to_pass_states.append(tag_state)\n",
    "\n",
    "\n",
    "basic_model.add_states()    \n",
    "    \n",
    "\n",
    "start_prob={}\n",
    "\n",
    "for tag in tags:\n",
    "    start_prob[tag]=starting_tag_count[tag]/tags_count[tag]\n",
    "\n",
    "for tag_state in to_pass_states :\n",
    "    basic_model.add_transition(basic_model.start,tag_state,start_prob[tag_state.name])    \n",
    "\n",
    "end_prob={}\n",
    "\n",
    "for tag in tags:\n",
    "    end_prob[tag]=ending_tag_count[tag]/tags_count[tag]\n",
    "for tag_state in to_pass_states :\n",
    "    basic_model.add_transition(tag_state,basic_model.end,end_prob[tag_state.name])\n",
    "    \n",
    "\n",
    "\n",
    "transition_prob_pair={}\n",
    "\n",
    "for key in tag_bigrams.keys():\n",
    "    transition_prob_pair[key]=tag_bigrams.get(key)/tags_count[key[0]]\n",
    "for tag_state in to_pass_states :\n",
    "    for next_tag_state in to_pass_states :\n",
    "        basic_model.add_transition(tag_state,next_tag_state,transition_prob_pair[(tag_state.name,next_tag_state.name)])\n",
    "\n",
    "basic_model.bake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic HMM Model Training Accuracy is: 97.49%\n",
      "Basic HMM Model Testing Accuracy is: 96.09%\n"
     ]
    }
   ],
   "source": [
    "#Model Accuracy Evaluation\n",
    "\n",
    "hmm_training_acc = modelaccuracy(data.training_set.X, data.training_set.Y, basic_model)\n",
    "print(\"Basic HMM Model Training Accuracy is: {:.2f}%\".format(100 * hmm_training_acc))\n",
    "\n",
    "hmm_testing_acc = modelaccuracy(data.testing_set.X, data.testing_set.Y, basic_model)\n",
    "print(\"Basic HMM Model Testing Accuracy is: {:.2f}%\".format(100 * hmm_testing_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Key: b100-28144\n",
      "\n",
      "Here are predicted labels:\n",
      "\n",
      "['CONJ', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'CONJ', 'NOUN', 'NUM', '.', '.', 'NOUN', '.', '.']\n",
      "\n",
      "\n",
      "Here are Actual labels:\n",
      "\n",
      "('CONJ', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'CONJ', 'NOUN', 'NUM', '.', '.', 'NOUN', '.', '.')\n",
      "\n",
      "\n",
      "Sentence Key: b100-23146\n",
      "\n",
      "Here are predicted labels:\n",
      "\n",
      "['PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'ADJ', 'ADJ', 'NOUN', 'VERB', 'VERB', '.', 'ADP', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADP', 'DET', 'NOUN', '.']\n",
      "\n",
      "\n",
      "Here are Actual labels:\n",
      "\n",
      "('PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'ADJ', 'ADJ', 'NOUN', 'VERB', 'VERB', '.', 'ADP', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADP', 'DET', 'NOUN', '.')\n",
      "\n",
      "\n",
      "Sentence Key: b100-35462\n",
      "\n",
      "Here are predicted labels:\n",
      "\n",
      "['DET', 'ADJ', 'NOUN', 'VERB', 'VERB', 'VERB', 'ADP', 'DET', 'ADJ', 'ADJ', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', '.', 'ADP', 'ADJ', 'NOUN', '.', 'CONJ', 'ADP', 'DET', 'NOUN', 'ADP', 'ADJ', 'ADJ', '.', 'ADJ', '.', 'CONJ', 'ADJ', 'NOUN', 'ADP', 'ADJ', 'NOUN', '.']\n",
      "\n",
      "\n",
      "Here are Actual labels:\n",
      "\n",
      "('DET', 'ADJ', 'NOUN', 'VERB', 'VERB', 'VERB', 'ADP', 'DET', 'ADJ', 'ADJ', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', '.', 'ADP', 'ADJ', 'NOUN', '.', 'CONJ', 'ADP', 'DET', 'NOUN', 'ADP', 'ADJ', 'ADJ', '.', 'ADJ', '.', 'CONJ', 'ADJ', 'NOUN', 'ADP', 'ADJ', 'NOUN', '.')\n",
      "\n",
      "\n",
      "Sentence Key: b100-37008\n",
      "\n",
      "Here are predicted labels:\n",
      "\n",
      "['PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', '.', 'ADP', 'ADJ', 'NOUN', '.', 'ADP', 'DET', 'NOUN', 'ADP', 'NOUN', '.']\n",
      "\n",
      "\n",
      "Here are Actual labels:\n",
      "\n",
      "('PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', '.', 'ADP', 'ADJ', 'NOUN', '.', 'ADP', 'DET', 'NOUN', 'ADP', 'NOUN', '.')\n",
      "\n",
      "\n",
      "Sentence Key: b100-18135\n",
      "\n",
      "Here are predicted labels:\n",
      "\n",
      "['CONJ', 'PRON', 'VERB', 'VERB', '.']\n",
      "\n",
      "\n",
      "Here are Actual labels:\n",
      "\n",
      "('CONJ', 'NOUN', 'VERB', 'VERB', '.')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Adds code blocks wherever you feel necessary\n",
    "for key in data.testing_set.keys[:5]:\n",
    "    print(\"Sentence Key: {}\\n\".format(key))\n",
    "    print(\"Here are predicted labels:\\n\")\n",
    "    print(simplification_decoding(data.sentences[key].words, basic_model))\n",
    "    print(\"\\n\")\n",
    "    print(\"Here are Actual labels:\\n\")\n",
    "    print(data.sentences[key].tags)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Happy Coding!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
